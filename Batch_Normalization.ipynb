{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization – lab\n",
    "[from Udacity's DLND] (https://www.udacity.com/course/deep-learning-nanodegree--nd101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MNIST classifier \n",
    "* Neural network with 20 convolutional layers\n",
    "* Fully connected layer\n",
    "\n",
    "Although this is not a good architecture for MNIST classifier, it's a good example to illustrate batch-norm\n",
    "\n",
    "1. Complicated enough that training would benefit from batch normalization.\n",
    "2. Simple enough that it would train quickly, since this is meant to be a short exercise just to give you some practice adding batch normalization.\n",
    "3. Simple enough that the architecture would be easy to understand without additional resources.\n",
    "\n",
    "More about Batch-norm https://padlet.com/nvmoyar/9dfpaqb93c4e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes two versions of the network: \n",
    "\n",
    "1. [Batch Normalization with `tf.layers.batch_normalization`](#example_1) --> higher abstraction level package\n",
    "2. [Batch Normalization with `tf.nn.batch_normalization`](#example_2) --> lower abstraction level package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads TensorFlow, downloads the MNIST dataset if necessary, and loads it into an object named `mnist`. You'll need to run this cell before running anything else in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model without batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create convolutional layers in our network. They are very basic: \n",
    "\n",
    "* 3x3 kernel\n",
    "* ReLU activation functions\n",
    "* strides of 1x1 on layers with odd depths\n",
    "* and strides of 2x2 on layers with even depths\n",
    "\n",
    "We aren't bothering with pooling layers at all in this network.\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    # if the depth is divisible by 3, then strides =2 and therefore, we have a half-sized image, otherwise same size\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following cell**, along with the earlier cells (to load the dataset and define the necessary functions). \n",
    "\n",
    "This cell builds the network **without** batch normalization, then trains it on the MNIST dataset. It displays loss and accuracy data periodically while training.\n",
    "\n",
    "### Shapes\n",
    "\n",
    "We will run a 20 convolutional layers, and we set depth as layer_i*4 which is a way of making the shapes to change automatically. We need to bear in mind that shapes are related to strides in conv_layer() function. If depth is divisible by 3, stride is 2 which means that every 3 loops, inputs change size. \n",
    "\n",
    "So the shapes will be:\n",
    "\n",
    "* 1 (?, 28, 28, 4)\n",
    "* 2 (?, 28, 28, 8)\n",
    "* 3 (?, 14, 14, 12)\n",
    "* 4 (?, 14, 14, 16)\n",
    "* 5 (?, 14, 14, 20)\n",
    "* 6 (?, 7, 7, 24)\n",
    "* 7 (?, 7, 7, 28)\n",
    "* 8 (?, 7, 7, 32)\n",
    "* 9 (?, 4, 4, 36)\n",
    "* 10 (?, 4, 4, 40)\n",
    "* 11 (?, 4, 4, 44)\n",
    "* 12 (?, 2, 2, 48)\n",
    "* 13 (?, 2, 2, 52)\n",
    "* 14 (?, 2, 2, 56)\n",
    "* 15 (?, 1, 1, 60)\n",
    "* 16 (?, 1, 1, 64)\n",
    "* 17 (?, 1, 1, 68)\n",
    "* 18 (?, 1, 1, 72)\n",
    "* 19 (?, 1, 1, 76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69137, Validation accuracy: 0.09580\n",
      "Batch: 25: Training loss: 0.33708, Training accuracy: 0.09375\n",
      "Batch: 50: Training loss: 0.32516, Training accuracy: 0.04688\n",
      "Batch: 75: Training loss: 0.32335, Training accuracy: 0.26562\n",
      "Batch: 100: Validation loss: 0.32559, Validation accuracy: 0.11260\n",
      "Batch: 125: Training loss: 0.32436, Training accuracy: 0.14062\n",
      "Batch: 150: Training loss: 0.32702, Training accuracy: 0.01562\n",
      "Batch: 175: Training loss: 0.32555, Training accuracy: 0.10938\n",
      "Batch: 200: Validation loss: 0.32592, Validation accuracy: 0.11000\n",
      "Batch: 225: Training loss: 0.32514, Training accuracy: 0.15625\n",
      "Batch: 250: Training loss: 0.32563, Training accuracy: 0.09375\n",
      "Batch: 275: Training loss: 0.32642, Training accuracy: 0.07812\n",
      "Batch: 300: Validation loss: 0.32616, Validation accuracy: 0.11000\n",
      "Batch: 325: Training loss: 0.32585, Training accuracy: 0.09375\n",
      "Batch: 350: Training loss: 0.32257, Training accuracy: 0.15625\n",
      "Batch: 375: Training loss: 0.32602, Training accuracy: 0.06250\n",
      "Batch: 400: Validation loss: 0.32518, Validation accuracy: 0.09900\n",
      "Batch: 425: Training loss: 0.32568, Training accuracy: 0.06250\n",
      "Batch: 450: Training loss: 0.32566, Training accuracy: 0.10938\n",
      "Batch: 475: Training loss: 0.32443, Training accuracy: 0.12500\n",
      "Batch: 500: Validation loss: 0.32589, Validation accuracy: 0.11000\n",
      "Batch: 525: Training loss: 0.32768, Training accuracy: 0.04688\n",
      "Batch: 550: Training loss: 0.32443, Training accuracy: 0.07812\n",
      "Batch: 575: Training loss: 0.32672, Training accuracy: 0.04688\n",
      "Batch: 600: Validation loss: 0.32550, Validation accuracy: 0.11260\n",
      "Batch: 625: Training loss: 0.32430, Training accuracy: 0.09375\n",
      "Batch: 650: Training loss: 0.32714, Training accuracy: 0.14062\n",
      "Batch: 675: Training loss: 0.32633, Training accuracy: 0.10938\n",
      "Batch: 700: Validation loss: 0.32515, Validation accuracy: 0.11000\n",
      "Batch: 725: Training loss: 0.32845, Training accuracy: 0.06250\n",
      "Batch: 750: Training loss: 0.32588, Training accuracy: 0.10938\n",
      "Batch: 775: Training loss: 0.32769, Training accuracy: 0.04688\n",
      "Final validation accuracy: 0.09900\n",
      "Final test accuracy: 0.10090\n",
      "Accuracy on 100 samples: 0.11\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    \n",
    "    # BUILD THE MODEL #########################################\n",
    "    \n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    \n",
    "    layer = inputs  # inputs  (?, 28, 28, 1)\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i) # we use layer_i to apply depths automatically\n",
    "                  \n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list() # [None, 1, 1, 76]\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]]) # after reshape  (?, 76)\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100) # (?, 100)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10) # shape=(?, 10)\n",
    "    \n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # END OF THE MODEL ###############################################\n",
    "    \n",
    "    # TRAIN AND TEST THE MODEL #######################################\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches): # now we define batches \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Final validation accuracy: 0.09900\n",
    "* Final test accuracy: 0.10090\n",
    "* Accuracy on 100 samples: 0.11\n",
    "\n",
    "\n",
    "## Adding batch normalization and improving the previous results\n",
    "\n",
    "For this exercise, we will use [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) to handle most of the math, but you'll need to make a few other changes to your network to integrate batch normalization. \n",
    "\n",
    "Batch Normalization using `tf.layers`  [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization)\n",
    "\n",
    "To add batch normalization to the layers created by conv_layer, we do the following:\n",
    "\n",
    "* Added the is_training parameter to the function signature so we can pass that information to the batch normalization layer.\n",
    "* Removed the bias and activation function from the conv2d layer.\n",
    "* Used tf.layers.batch_normalization to normalize the convolutional layer's output. Notice we pass is_training to this layer to ensure the network updates its population statistics appropriately.\n",
    "* Passed the normalized values into a ReLU activation function.\n",
    "\n",
    "Batch normalization is still a new enough idea that researchers are still discovering how best to use it. In general, people seem to agree to remove the layer's bias (because the batch normalization already has terms for scaling and shifting) and add batch normalization before the layer's non-linear activation function. However, for some networks it will work well in other ways, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "  \n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the train\n",
    "\n",
    "* Add is_training, a placeholder to store a boolean value indicating whether or not the network is training.\n",
    "* Pass is_training parameter to the conv_layer and fully_connected functions, as expected. \n",
    "* Each time we call run on the session, we added to feed_dict the appropriate value for **is_training**.\n",
    "\n",
    "* In addition to that code, the training step is wrapped in the following with statement:\n",
    "\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    This line actually works in conjunction with the training parameter we pass to tf.layers.batch_normalization. Without it, TensorFlow's batch normalization layer will not operate correctly during inference.\n",
    "\n",
    "    Finally, whenever we train the network or perform inference, we use the feed_dict to set self.is_training to True or False, respectively, like in the following line:\n",
    "\n",
    "    session.run(train_step, feed_dict={self.input_layer: batch_xs, labels: batch_ys, self.is_training: True})\n",
    "    We'll go into more details later, but next we want to show some experiments that use this code and test networks with and without batch normalization.\n",
    "\n",
    "    Moved the creation of train_opt inside a with tf.control_dependencies... statement. This is necessary to get the normalization layers created with tf.layers.batch_normalization to update their population statistics, which we need when performing inference. Without this context update, the inference won't work properly and low accuracies will be\n",
    "    achieved instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69091, Validation accuracy: 0.09760\n",
      "Batch: 25: Training loss: 0.59167, Training accuracy: 0.07812\n",
      "Batch: 50: Training loss: 0.49184, Training accuracy: 0.07812\n",
      "Batch: 75: Training loss: 0.42548, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.37773, Validation accuracy: 0.09240\n",
      "Batch: 125: Training loss: 0.35851, Training accuracy: 0.06250\n",
      "Batch: 150: Training loss: 0.33854, Training accuracy: 0.10938\n",
      "Batch: 175: Training loss: 0.32412, Training accuracy: 0.15625\n",
      "Batch: 200: Validation loss: 0.29513, Validation accuracy: 0.25780\n",
      "Batch: 225: Training loss: 0.24947, Training accuracy: 0.45312\n",
      "Batch: 250: Training loss: 0.23215, Training accuracy: 0.50000\n",
      "Batch: 275: Training loss: 0.28767, Training accuracy: 0.42188\n",
      "Batch: 300: Validation loss: 0.17280, Validation accuracy: 0.64820\n",
      "Batch: 325: Training loss: 0.10398, Training accuracy: 0.78125\n",
      "Batch: 350: Training loss: 0.14164, Training accuracy: 0.73438\n",
      "Batch: 375: Training loss: 0.13091, Training accuracy: 0.78125\n",
      "Batch: 400: Validation loss: 0.16291, Validation accuracy: 0.79560\n",
      "Batch: 425: Training loss: 0.10683, Training accuracy: 0.87500\n",
      "Batch: 450: Training loss: 0.14042, Training accuracy: 0.81250\n",
      "Batch: 475: Training loss: 0.10805, Training accuracy: 0.84375\n",
      "Batch: 500: Validation loss: 0.02953, Validation accuracy: 0.95480\n",
      "Batch: 525: Training loss: 0.17700, Training accuracy: 0.75000\n",
      "Batch: 550: Training loss: 0.01139, Training accuracy: 0.98438\n",
      "Batch: 575: Training loss: 0.01907, Training accuracy: 0.96875\n",
      "Batch: 600: Validation loss: 0.03984, Validation accuracy: 0.94360\n",
      "Batch: 625: Training loss: 0.00319, Training accuracy: 1.00000\n",
      "Batch: 650: Training loss: 0.05514, Training accuracy: 0.92188\n",
      "Batch: 675: Training loss: 0.00283, Training accuracy: 1.00000\n",
      "Batch: 700: Validation loss: 0.04061, Validation accuracy: 0.94480\n",
      "Batch: 725: Training loss: 0.05661, Training accuracy: 0.93750\n",
      "Batch: 750: Training loss: 0.03027, Training accuracy: 0.95312\n",
      "Batch: 775: Training loss: 0.03633, Training accuracy: 0.95312\n",
      "Final validation accuracy: 0.92800\n",
      "Final test accuracy: 0.91980\n",
      "Accuracy on 100 samples: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Edit the train function to support batch normalization and make sure it updates and uses its population \n",
    "# statistics correctly.\n",
    "\n",
    "\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs \n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training) # is_training\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)  # is_training\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    # Tell TensorFlow to update the population statistics while training\n",
    "    # A context manager that specifies control dependencies for all operations constructed within the context.\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)   # !IMPORTANT\n",
    "       \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})  # is_training = Value\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy, every 100 epochs\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels, is_training: False}) # now we are not training, we are validating\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels, is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]], is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch normalization, you should now get an accuracy over 90%. Notice also the last line of the output: `Accuracy on 100 samples`. If this value is low while everything else looks good, that means you did not implement batch normalization correctly. Specifically, it means you either did not calculate the population mean and variance while training, or you are not using those values during inference.\n",
    "\n",
    "# Batch Normalization using `tf.nn.batch_normalization`<a id=\"example_2\"></a>\n",
    "\n",
    "Most of the time you will be able to use higher level functions exclusively, but sometimes you may want to work at a lower level. For example, if you ever want to implement a new feature – something new enough that TensorFlow does not already include a high-level implementation of it, like batch normalization in an LSTM – then you may need to know these sorts of things.\n",
    "\n",
    "This version of the network uses `tf.nn` for almost everything, and expects you to implement batch normalization using [`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization).\n",
    "\n",
    "**Optional TODO:** You can run the next three cells before you edit them just to see how the network performs without batch normalization. However, the results should be pretty much the same as you saw with the previous example before you added batch normalization. \n",
    "\n",
    "**TODO:** Modify `fully_connected` to add batch normalization to the fully connected layers it creates. Feel free to change the function's parameters if it helps.\n",
    "\n",
    "**Note:** For convenience, we continue to use `tf.layers.dense` for the `fully_connected` layer. By this point in the class, you should have no problem replacing that with matrix operations between the `prev_layer` and explicit weights and biases variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Modify `conv_layer` to add batch normalization to the fully connected layers it creates. Feel free to change the function's parameters if it helps.\n",
    "\n",
    "**Note:** Unlike in the previous example that used `tf.layers`, adding batch normalization to these convolutional layers _does_ require some slight differences to what you did in `fully_connected`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "\n",
    "    in_channels = prev_layer.get_shape().as_list()[3]\n",
    "    out_channels = layer_depth*4\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([3, 3, in_channels, out_channels], stddev=0.05))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(out_channels))\n",
    "\n",
    "    conv_layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Edit the `train` function to support batch normalization. You'll need to make sure the network knows whether or not it is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
